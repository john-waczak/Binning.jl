{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux \n",
    "using DataFrames, CSV\n",
    "using ProgressMeter\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Oz_freq_0Hz</th><th>Oz_freq_0_9765625Hz</th><th>Oz_freq_1_953125Hz</th><th>Oz_freq_2_929688Hz</th><th>Oz_freq_3_90625Hz</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>33,094 rows × 263 columns (omitted printing of 258 columns)</p><tr><th>1</th><td>2.96607e8</td><td>1.17803e8</td><td>11148.5</td><td>213.96</td><td>2336.42</td></tr><tr><th>2</th><td>2.96603e8</td><td>1.17803e8</td><td>11154.7</td><td>211.555</td><td>2340.26</td></tr><tr><th>3</th><td>2.96599e8</td><td>1.17803e8</td><td>11175.0</td><td>211.271</td><td>2338.25</td></tr><tr><th>4</th><td>2.96594e8</td><td>1.17802e8</td><td>11206.9</td><td>213.427</td><td>2332.51</td></tr><tr><th>5</th><td>2.96588e8</td><td>1.17802e8</td><td>11249.5</td><td>217.809</td><td>2324.85</td></tr><tr><th>6</th><td>2.96582e8</td><td>1.178e8</td><td>11299.6</td><td>223.96</td><td>2317.49</td></tr><tr><th>7</th><td>2.96575e8</td><td>1.17798e8</td><td>11356.0</td><td>230.965</td><td>2310.92</td></tr><tr><th>8</th><td>2.96568e8</td><td>1.17796e8</td><td>11406.7</td><td>239.1</td><td>2308.89</td></tr><tr><th>9</th><td>2.9656e8</td><td>1.17793e8</td><td>11452.2</td><td>246.736</td><td>2308.4</td></tr><tr><th>10</th><td>2.96552e8</td><td>1.17789e8</td><td>11489.9</td><td>252.802</td><td>2307.28</td></tr><tr><th>11</th><td>2.96544e8</td><td>1.17785e8</td><td>11511.2</td><td>257.356</td><td>2306.31</td></tr><tr><th>12</th><td>2.96535e8</td><td>1.17779e8</td><td>11512.9</td><td>259.998</td><td>2305.23</td></tr><tr><th>13</th><td>2.96526e8</td><td>1.17773e8</td><td>11499.2</td><td>259.698</td><td>2301.85</td></tr><tr><th>14</th><td>2.96516e8</td><td>1.17766e8</td><td>11462.6</td><td>257.579</td><td>2300.83</td></tr><tr><th>15</th><td>2.96506e8</td><td>1.17758e8</td><td>11408.2</td><td>253.447</td><td>2302.07</td></tr><tr><th>16</th><td>2.96497e8</td><td>1.1775e8</td><td>11342.5</td><td>247.249</td><td>2304.85</td></tr><tr><th>17</th><td>2.96488e8</td><td>1.17741e8</td><td>11268.0</td><td>239.884</td><td>2309.93</td></tr><tr><th>18</th><td>2.96478e8</td><td>1.17732e8</td><td>11182.9</td><td>232.973</td><td>2319.09</td></tr><tr><th>19</th><td>2.96469e8</td><td>1.17723e8</td><td>11098.4</td><td>226.285</td><td>2327.38</td></tr><tr><th>20</th><td>2.9646e8</td><td>1.17715e8</td><td>11018.9</td><td>220.51</td><td>2332.81</td></tr><tr><th>21</th><td>2.96451e8</td><td>1.17707e8</td><td>10946.3</td><td>216.492</td><td>2335.2</td></tr><tr><th>22</th><td>2.96442e8</td><td>1.177e8</td><td>10884.3</td><td>214.558</td><td>2334.05</td></tr><tr><th>23</th><td>2.96434e8</td><td>1.17693e8</td><td>10839.8</td><td>214.189</td><td>2328.1</td></tr><tr><th>24</th><td>2.96427e8</td><td>1.17687e8</td><td>10814.5</td><td>215.268</td><td>2318.89</td></tr><tr><th>25</th><td>2.9642e8</td><td>1.17683e8</td><td>10806.9</td><td>217.785</td><td>2309.64</td></tr><tr><th>26</th><td>2.96413e8</td><td>1.17679e8</td><td>10817.1</td><td>221.245</td><td>2302.07</td></tr><tr><th>27</th><td>2.96408e8</td><td>1.17677e8</td><td>10842.4</td><td>225.317</td><td>2298.12</td></tr><tr><th>28</th><td>2.96402e8</td><td>1.17676e8</td><td>10881.0</td><td>229.441</td><td>2297.84</td></tr><tr><th>29</th><td>2.96398e8</td><td>1.17675e8</td><td>10931.1</td><td>233.152</td><td>2300.33</td></tr><tr><th>30</th><td>2.96394e8</td><td>1.17676e8</td><td>10984.5</td><td>236.903</td><td>2306.9</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& Oz\\_freq\\_0Hz & Oz\\_freq\\_0\\_9765625Hz & Oz\\_freq\\_1\\_953125Hz & Oz\\_freq\\_2\\_929688Hz & Oz\\_freq\\_3\\_90625Hz & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 2.96607e8 & 1.17803e8 & 11148.5 & 213.96 & 2336.42 & $\\dots$ \\\\\n",
       "\t2 & 2.96603e8 & 1.17803e8 & 11154.7 & 211.555 & 2340.26 & $\\dots$ \\\\\n",
       "\t3 & 2.96599e8 & 1.17803e8 & 11175.0 & 211.271 & 2338.25 & $\\dots$ \\\\\n",
       "\t4 & 2.96594e8 & 1.17802e8 & 11206.9 & 213.427 & 2332.51 & $\\dots$ \\\\\n",
       "\t5 & 2.96588e8 & 1.17802e8 & 11249.5 & 217.809 & 2324.85 & $\\dots$ \\\\\n",
       "\t6 & 2.96582e8 & 1.178e8 & 11299.6 & 223.96 & 2317.49 & $\\dots$ \\\\\n",
       "\t7 & 2.96575e8 & 1.17798e8 & 11356.0 & 230.965 & 2310.92 & $\\dots$ \\\\\n",
       "\t8 & 2.96568e8 & 1.17796e8 & 11406.7 & 239.1 & 2308.89 & $\\dots$ \\\\\n",
       "\t9 & 2.9656e8 & 1.17793e8 & 11452.2 & 246.736 & 2308.4 & $\\dots$ \\\\\n",
       "\t10 & 2.96552e8 & 1.17789e8 & 11489.9 & 252.802 & 2307.28 & $\\dots$ \\\\\n",
       "\t11 & 2.96544e8 & 1.17785e8 & 11511.2 & 257.356 & 2306.31 & $\\dots$ \\\\\n",
       "\t12 & 2.96535e8 & 1.17779e8 & 11512.9 & 259.998 & 2305.23 & $\\dots$ \\\\\n",
       "\t13 & 2.96526e8 & 1.17773e8 & 11499.2 & 259.698 & 2301.85 & $\\dots$ \\\\\n",
       "\t14 & 2.96516e8 & 1.17766e8 & 11462.6 & 257.579 & 2300.83 & $\\dots$ \\\\\n",
       "\t15 & 2.96506e8 & 1.17758e8 & 11408.2 & 253.447 & 2302.07 & $\\dots$ \\\\\n",
       "\t16 & 2.96497e8 & 1.1775e8 & 11342.5 & 247.249 & 2304.85 & $\\dots$ \\\\\n",
       "\t17 & 2.96488e8 & 1.17741e8 & 11268.0 & 239.884 & 2309.93 & $\\dots$ \\\\\n",
       "\t18 & 2.96478e8 & 1.17732e8 & 11182.9 & 232.973 & 2319.09 & $\\dots$ \\\\\n",
       "\t19 & 2.96469e8 & 1.17723e8 & 11098.4 & 226.285 & 2327.38 & $\\dots$ \\\\\n",
       "\t20 & 2.9646e8 & 1.17715e8 & 11018.9 & 220.51 & 2332.81 & $\\dots$ \\\\\n",
       "\t21 & 2.96451e8 & 1.17707e8 & 10946.3 & 216.492 & 2335.2 & $\\dots$ \\\\\n",
       "\t22 & 2.96442e8 & 1.177e8 & 10884.3 & 214.558 & 2334.05 & $\\dots$ \\\\\n",
       "\t23 & 2.96434e8 & 1.17693e8 & 10839.8 & 214.189 & 2328.1 & $\\dots$ \\\\\n",
       "\t24 & 2.96427e8 & 1.17687e8 & 10814.5 & 215.268 & 2318.89 & $\\dots$ \\\\\n",
       "\t25 & 2.9642e8 & 1.17683e8 & 10806.9 & 217.785 & 2309.64 & $\\dots$ \\\\\n",
       "\t26 & 2.96413e8 & 1.17679e8 & 10817.1 & 221.245 & 2302.07 & $\\dots$ \\\\\n",
       "\t27 & 2.96408e8 & 1.17677e8 & 10842.4 & 225.317 & 2298.12 & $\\dots$ \\\\\n",
       "\t28 & 2.96402e8 & 1.17676e8 & 10881.0 & 229.441 & 2297.84 & $\\dots$ \\\\\n",
       "\t29 & 2.96398e8 & 1.17675e8 & 10931.1 & 233.152 & 2300.33 & $\\dots$ \\\\\n",
       "\t30 & 2.96394e8 & 1.17676e8 & 10984.5 & 236.903 & 2306.9 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m33094×263 DataFrame\u001b[0m\n",
       "\u001b[1m   Row \u001b[0m│\u001b[1m Oz_freq_0Hz \u001b[0m\u001b[1m Oz_freq_0_9765625Hz \u001b[0m\u001b[1m Oz_freq_1_953125Hz \u001b[0m\u001b[1m Oz_freq_2_92968\u001b[0m ⋯\n",
       "\u001b[1m       \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64             \u001b[0m\u001b[90m Float64            \u001b[0m\u001b[90m Float64        \u001b[0m ⋯\n",
       "───────┼────────────────────────────────────────────────────────────────────────\n",
       "     1 │   2.96607e8            1.17803e8            11148.5             213.9 ⋯\n",
       "     2 │   2.96603e8            1.17803e8            11154.7             211.5\n",
       "     3 │   2.96599e8            1.17803e8            11175.0             211.2\n",
       "     4 │   2.96594e8            1.17802e8            11206.9             213.4\n",
       "     5 │   2.96588e8            1.17802e8            11249.5             217.8 ⋯\n",
       "     6 │   2.96582e8            1.178e8              11299.6             223.9\n",
       "     7 │   2.96575e8            1.17798e8            11356.0             230.9\n",
       "     8 │   2.96568e8            1.17796e8            11406.7             239.1\n",
       "     9 │   2.9656e8             1.17793e8            11452.2             246.7 ⋯\n",
       "    10 │   2.96552e8            1.17789e8            11489.9             252.8\n",
       "    11 │   2.96544e8            1.17785e8            11511.2             257.3\n",
       "   ⋮   │      ⋮                ⋮                   ⋮                   ⋮       ⋱\n",
       " 33085 │   1.27056e8            5.04735e7             4958.23            109.2\n",
       " 33086 │   1.27061e8            5.04745e7             4959.36            111.7 ⋯\n",
       " 33087 │   1.27065e8            5.0475e7              4954.45            113.1\n",
       " 33088 │   1.27068e8            5.04751e7             4943.81            113.1\n",
       " 33089 │   1.27071e8            5.04748e7             4926.9             111.7\n",
       " 33090 │   1.27074e8            5.04741e7             4905.11            109.0 ⋯\n",
       " 33091 │   1.27077e8            5.04731e7             4879.28            105.4\n",
       " 33092 │   1.2708e8             5.04718e7             4851.49            101.4\n",
       " 33093 │   1.27082e8            5.04704e7             4821.85             97.7\n",
       " 33094 │   1.27084e8            5.04688e7             4794.24             94.5 ⋯\n",
       "\u001b[36m                                              260 columns and 33073 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Oz_path = \"./data/Oz.csv\"\n",
    "Oz_csv = CSV.File(Oz_path)\n",
    "Oz_df = DataFrame(Oz_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Left_EAR</th><th>Right_EAR</th></tr><tr><th></th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>33,094 rows × 2 columns</p><tr><th>1</th><td>0.297786</td><td>0.27874</td></tr><tr><th>2</th><td>0.267438</td><td>0.277778</td></tr><tr><th>3</th><td>0.285194</td><td>0.280247</td></tr><tr><th>4</th><td>0.268442</td><td>0.277778</td></tr><tr><th>5</th><td>0.287206</td><td>0.267686</td></tr><tr><th>6</th><td>0.287206</td><td>0.267686</td></tr><tr><th>7</th><td>0.2687</td><td>0.278931</td></tr><tr><th>8</th><td>0.297786</td><td>0.267857</td></tr><tr><th>9</th><td>0.296776</td><td>0.278931</td></tr><tr><th>10</th><td>0.321566</td><td>0.297245</td></tr><tr><th>11</th><td>0.321566</td><td>0.314815</td></tr><tr><th>12</th><td>0.322585</td><td>0.296296</td></tr><tr><th>13</th><td>0.309647</td><td>0.30456</td></tr><tr><th>14</th><td>0.321566</td><td>0.333105</td></tr><tr><th>15</th><td>0.249839</td><td>0.240741</td></tr><tr><th>16</th><td>0.2687</td><td>0.22207</td></tr><tr><th>17</th><td>0.303933</td><td>0.314599</td></tr><tr><th>18</th><td>0.303933</td><td>0.316993</td></tr><tr><th>19</th><td>0.2687</td><td>0.260397</td></tr><tr><th>20</th><td>0.351637</td><td>0.347219</td></tr><tr><th>21</th><td>0.313909</td><td>0.307465</td></tr><tr><th>22</th><td>0.315055</td><td>0.296093</td></tr><tr><th>23</th><td>0.32862</td><td>0.303571</td></tr><tr><th>24</th><td>0.302949</td><td>0.277587</td></tr><tr><th>25</th><td>0.321566</td><td>0.314599</td></tr><tr><th>26</th><td>0.321566</td><td>0.297245</td></tr><tr><th>27</th><td>0.33824</td><td>0.296093</td></tr><tr><th>28</th><td>0.339224</td><td>0.314599</td></tr><tr><th>29</th><td>0.297786</td><td>0.288462</td></tr><tr><th>30</th><td>0.297786</td><td>0.288462</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& Left\\_EAR & Right\\_EAR\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 0.297786 & 0.27874 \\\\\n",
       "\t2 & 0.267438 & 0.277778 \\\\\n",
       "\t3 & 0.285194 & 0.280247 \\\\\n",
       "\t4 & 0.268442 & 0.277778 \\\\\n",
       "\t5 & 0.287206 & 0.267686 \\\\\n",
       "\t6 & 0.287206 & 0.267686 \\\\\n",
       "\t7 & 0.2687 & 0.278931 \\\\\n",
       "\t8 & 0.297786 & 0.267857 \\\\\n",
       "\t9 & 0.296776 & 0.278931 \\\\\n",
       "\t10 & 0.321566 & 0.297245 \\\\\n",
       "\t11 & 0.321566 & 0.314815 \\\\\n",
       "\t12 & 0.322585 & 0.296296 \\\\\n",
       "\t13 & 0.309647 & 0.30456 \\\\\n",
       "\t14 & 0.321566 & 0.333105 \\\\\n",
       "\t15 & 0.249839 & 0.240741 \\\\\n",
       "\t16 & 0.2687 & 0.22207 \\\\\n",
       "\t17 & 0.303933 & 0.314599 \\\\\n",
       "\t18 & 0.303933 & 0.316993 \\\\\n",
       "\t19 & 0.2687 & 0.260397 \\\\\n",
       "\t20 & 0.351637 & 0.347219 \\\\\n",
       "\t21 & 0.313909 & 0.307465 \\\\\n",
       "\t22 & 0.315055 & 0.296093 \\\\\n",
       "\t23 & 0.32862 & 0.303571 \\\\\n",
       "\t24 & 0.302949 & 0.277587 \\\\\n",
       "\t25 & 0.321566 & 0.314599 \\\\\n",
       "\t26 & 0.321566 & 0.297245 \\\\\n",
       "\t27 & 0.33824 & 0.296093 \\\\\n",
       "\t28 & 0.339224 & 0.314599 \\\\\n",
       "\t29 & 0.297786 & 0.288462 \\\\\n",
       "\t30 & 0.297786 & 0.288462 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m33094×2 DataFrame\u001b[0m\n",
       "\u001b[1m   Row \u001b[0m│\u001b[1m Left_EAR \u001b[0m\u001b[1m Right_EAR \u001b[0m\n",
       "\u001b[1m       \u001b[0m│\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\n",
       "───────┼─────────────────────\n",
       "     1 │ 0.297786   0.27874\n",
       "     2 │ 0.267438   0.277778\n",
       "     3 │ 0.285194   0.280247\n",
       "     4 │ 0.268442   0.277778\n",
       "     5 │ 0.287206   0.267686\n",
       "     6 │ 0.287206   0.267686\n",
       "     7 │ 0.2687     0.278931\n",
       "     8 │ 0.297786   0.267857\n",
       "     9 │ 0.296776   0.278931\n",
       "    10 │ 0.321566   0.297245\n",
       "    11 │ 0.321566   0.314815\n",
       "   ⋮   │    ⋮          ⋮\n",
       " 33085 │ 0.296776   0.27874\n",
       " 33086 │ 0.289107   0.298398\n",
       " 33087 │ 0.278533   0.308661\n",
       " 33088 │ 0.296776   0.316776\n",
       " 33089 │ 0.29563    0.309858\n",
       " 33090 │ 0.313909   0.315751\n",
       " 33091 │ 0.306481   0.307465\n",
       " 33092 │ 0.295378   0.346196\n",
       " 33093 │ 0.308043   0.307465\n",
       " 33094 │ 0.327016   0.309858\n",
       "\u001b[36m           33073 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Oz_in = Oz_df[:, Between(:Oz_freq_0Hz,:Oz_freq_250Hz)]\n",
    "Oz_out = Oz_df[:, [:Left_EAR, :Right_EAR]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DomainBinner{StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}, Vector{Float64}, Float64, Float64}(0.0:0.9765625:250.0, [25.0, 50.0, 75.0, 100.0, 125.0, 150.0, 175.0, 200.0, 225.0], 0.0, 250.0)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = range(0, stop=250, length=257) # set up frequency range for initializing the binning layer\n",
    "b = DomainBinner(f, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test just the binning layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float64}:\n",
       " 12.259971198942178\n",
       "  9.870104740405495\n",
       " 10.049734970771752\n",
       " 12.339338002966958\n",
       " 12.372488498251872\n",
       " 13.277041588215837\n",
       " 14.308062353119483\n",
       " 12.945285293447146\n",
       " 14.200671542641542\n",
       " 17.102917167319298"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b(rand(257))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "again but with multidimensional datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to be able to combine the binning layer with a dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(10, 2)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Dense(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -0.7349031223728898\n",
       " -0.4636611845413584"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d(rand(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×20 Matrix{Float64}:\n",
       " -0.344268   -0.574984  -0.639127  …  -0.687086  -0.797017    -0.720975\n",
       "  0.0918396   0.266993   0.509647      0.559029   0.00836097   0.175824"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d(rand(10, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(DomainBinner{StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}}, Vector{Float64}, Float64, Float64}(0.0:0.9765625:250.0, [25.0, 50.0, 75.0, 100.0, 125.0, 150.0, 175.0, 200.0, 225.0], 0.0, 250.0), Dense(10, 2))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = DomainBinner(f, 10)\n",
    "D = Dense(10, 2)\n",
    "Model = Chain(B, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       "  10.20057238483849\n",
       " -19.00365202686174"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model(rand(257))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss(x, y) = Flux.Losses.mse(Model(x), y)\n",
    "loss(x, y) = sum((Model(x).-y).^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257, 33094)(2, 33094)\n"
     ]
    }
   ],
   "source": [
    "Xtrain = Array(transpose(Array(Oz_in)))\n",
    "Ytrain = Array(transpose(Array(Oz_out)))\n",
    "println(size(Xtrain), size(Ytrain))\n",
    "\n",
    "# take log of intensities \n",
    "Xtrain .= log.(Xtrain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batchSize = 100 \n",
    "idx = collect(1:batchSize:size(Xtrain)[2])\n",
    "push!(idx, size(Xtrain)[2])\n",
    "batch_idx = [idx[i]:idx[(i+1)] for i ∈ 1:length(idx)-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle the batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using Random:shuffle!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle!(batch_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xtest = Xtrain[:, batch_idx[1]]\n",
    "Ytest = Ytrain[:, batch_idx[1]]\n",
    "\n",
    "Model.(eachcol(Xtest))\n",
    "#model_test = [Model(col) for col ∈ eachcol(Xtest)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to see that the loss function works on a single instance of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59163.213556949566"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size(Xtrain[:,1])\n",
    "loss(Xtrain[:, 1], Ytrain[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, train the dense layer on the linearly initialized binner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Descent(0.001)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define optimizer\n",
    "opt = Descent(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[-0.6858388 0.47476384 … 0.5991513 0.30752262; -0.51307213 -0.4292729 … 0.10150192 -0.22462128], Float32[0.0, 0.0]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_binner = params(B)\n",
    "ps_dense = params(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65437.315028701894\n",
      "4.320632413449868e9\n",
      "2.8514617500396056e14\n",
      "1.8812595425670967e19\n",
      "1.241070542434324e24\n",
      "8.19587227851686e28\n",
      "5.41700580343688e33\n",
      "3.581842631090407e38\n",
      "2.370403687957522e43\n",
      "1.56933360221252e48\n",
      "1.03936609911075e53\n",
      "6.885320981272484e57\n",
      "4.561945494232538e62\n",
      "3.0247706086798465e67\n",
      "2.006810394161306e72\n",
      "1.3311835542065256e77\n",
      "8.826220177413317e81\n",
      "Inf\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] Array",
      "    @ ./boot.jl:448 [inlined]",
      "  [2] Array",
      "    @ ./boot.jl:457 [inlined]",
      "  [3] Array",
      "    @ ./boot.jl:465 [inlined]",
      "  [4] similar",
      "    @ ./abstractarray.jl:785 [inlined]",
      "  [5] similar",
      "    @ ./abstractarray.jl:784 [inlined]",
      "  [6] similar",
      "    @ ./broadcast.jl:197 [inlined]",
      "  [7] similar",
      "    @ ./broadcast.jl:196 [inlined]",
      "  [8] copy",
      "    @ ./broadcast.jl:908 [inlined]",
      "  [9] materialize",
      "    @ ./broadcast.jl:883 [inlined]",
      " [10] accum(x::Vector{Float64}, y::Vector{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/lib/lib.jl:16",
      " [11] macro expansion",
      "    @ ~/.julia/packages/Zygote/lwmfx/src/lib/lib.jl:18 [inlined]",
      " [12] accum",
      "    @ ~/.julia/packages/Zygote/lwmfx/src/lib/lib.jl:18 [inlined]",
      " [13] macro expansion",
      "    @ ./reduce.jl:245 [inlined]",
      " [14] macro expansion",
      "    @ ./simdloop.jl:77 [inlined]",
      " [15] mapreduce_impl(f::typeof(identity), op::typeof(Zygote.accum), A::Vector{NamedTuple{(:i, :f, :x, :b), Tuple{Nothing, Vector{Float64}, Vector{Float64}, Vector{Float64}}}}, ifirst::Int64, ilast::Int64, blksize::Int64)",
      "    @ Base ./reduce.jl:243",
      " [16] mapreduce_impl",
      "    @ ./reduce.jl:257 [inlined]",
      " [17] _mapreduce(f::typeof(identity), op::typeof(Zygote.accum), #unused#::IndexLinear, A::Vector{NamedTuple{(:i, :f, :x, :b), Tuple{Nothing, Vector{Float64}, Vector{Float64}, Vector{Float64}}}})",
      "    @ Base ./reduce.jl:415",
      " [18] _mapreduce_dim(f::Function, op::Function, #unused#::Base._InitialValue, A::Vector{NamedTuple{(:i, :f, :x, :b), Tuple{Nothing, Vector{Float64}, Vector{Float64}, Vector{Float64}}}}, #unused#::Colon)",
      "    @ Base ./reducedim.jl:318",
      " [19] #mapreduce#672",
      "    @ ./reducedim.jl:310 [inlined]",
      " [20] mapreduce",
      "    @ ./reducedim.jl:310 [inlined]",
      " [21] #reduce#674",
      "    @ ./reducedim.jl:359 [inlined]",
      " [22] reduce(op::Function, A::Vector{NamedTuple{(:i, :f, :x, :b), Tuple{Nothing, Vector{Float64}, Vector{Float64}, Vector{Float64}}}})",
      "    @ Base ./reducedim.jl:359",
      " [23] (::Zygote.var\"#550#554\"{Vector{typeof(∂(λ))}})(Δ::FillArrays.Fill{Float64, 1, Tuple{Base.OneTo{Int64}}})",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/lib/array.jl:209",
      " [24] (::Zygote.var\"#581#582\"{Zygote.var\"#550#554\"{Vector{typeof(∂(λ))}}})(ȳ::FillArrays.Fill{Float64, 1, Tuple{Base.OneTo{Int64}}})",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/lib/array.jl:239",
      " [25] Pullback",
      "    @ ./none:0 [inlined]",
      " [26] (::typeof(∂(λ)))(Δ::Float64)",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface2.jl:0",
      " [27] #551",
      "    @ ~/.julia/packages/Zygote/lwmfx/src/lib/array.jl:207 [inlined]",
      " [28] (::Base.var\"#4#5\"{Zygote.var\"#551#555\"})(a::Tuple{typeof(∂(λ)), Float64})",
      "    @ Base ./generator.jl:36",
      " [29] iterate",
      "    @ ./generator.jl:47 [inlined]",
      " [30] collect_to!(dest::Vector{Tuple{NamedTuple{(:f, :x, :b), Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}}}, Nothing}}, itr::Base.Generator{Base.Iterators.Zip{Tuple{Vector{typeof(∂(λ))}, Vector{Float64}}}, Base.var\"#4#5\"{Zygote.var\"#551#555\"}}, offs::Int64, st::Tuple{Int64, Int64})",
      "    @ Base ./array.jl:724",
      " [31] collect_to_with_first!(dest::Vector{Tuple{NamedTuple{(:f, :x, :b), Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}}}, Nothing}}, v1::Tuple{NamedTuple{(:f, :x, :b), Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}}}, Nothing}, itr::Base.Generator{Base.Iterators.Zip{Tuple{Vector{typeof(∂(λ))}, Vector{Float64}}}, Base.var\"#4#5\"{Zygote.var\"#551#555\"}}, st::Tuple{Int64, Int64})",
      "    @ Base ./array.jl:702",
      " [32] collect(itr::Base.Generator{Base.Iterators.Zip{Tuple{Vector{typeof(∂(λ))}, Vector{Float64}}}, Base.var\"#4#5\"{Zygote.var\"#551#555\"}})",
      "    @ Base ./array.jl:683",
      " [33] map",
      "    @ ./abstractarray.jl:2383 [inlined]",
      " [34] (::Zygote.var\"#550#554\"{Vector{typeof(∂(λ))}})(Δ::Vector{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/lib/array.jl:207",
      " [35] (::Zygote.var\"#581#582\"{Zygote.var\"#550#554\"{Vector{typeof(∂(λ))}}})(ȳ::Vector{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/lib/array.jl:239",
      " [36] Pullback",
      "    @ ~/.julia/packages/Binning/KnHZV/src/Binning.jl:68 [inlined]",
      " [37] (::typeof(∂(λ)))(Δ::Vector{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface2.jl:0",
      " [38] Pullback",
      "    @ ~/.julia/packages/Flux/qp1gc/src/layers/basic.jl:36 [inlined]",
      " [39] (::typeof(∂(applychain)))(Δ::Vector{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface2.jl:0",
      " [40] Pullback",
      "    @ ~/.julia/packages/Flux/qp1gc/src/layers/basic.jl:38 [inlined]",
      " [41] (::typeof(∂(λ)))(Δ::Vector{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface2.jl:0",
      " [42] Pullback",
      "    @ ./In[187]:2 [inlined]",
      " [43] (::typeof(∂(loss)))(Δ::Float64)",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface2.jl:0",
      " [44] Pullback",
      "    @ ./In[192]:7 [inlined]",
      " [45] (::typeof(∂(λ)))(Δ::Float64)",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface2.jl:0",
      " [46] (::Zygote.var\"#69#70\"{Zygote.Params, typeof(∂(λ)), Zygote.Context})(Δ::Float64)",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface.jl:252",
      " [47] gradient(f::Function, args::Zygote.Params)",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface.jl:59",
      " [48] top-level scope",
      "    @ In[192]:6",
      " [49] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [50] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1094"
     ]
    }
   ],
   "source": [
    "#Flux.train!(loss, ps, train_loader, opt)\n",
    "ctr = 1\n",
    "for i ∈ 1:size(Xtrain)[2]\n",
    "    x = Xtrain[:, i]\n",
    "    y = Ytrain[:, i]\n",
    "    gs = Flux.gradient(ps_dense) do \n",
    "        l = loss(x, y)    \n",
    "        println(l) \n",
    "        l\n",
    "    end \n",
    "    Flux.Optimise.update!(opt, ps_dense, gs)\n",
    "   # if ctr%500 == 0\n",
    "   #     println(ctr, \" \", loss(x,y))\n",
    "   # end\n",
    "    ctr += 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Descent(0.0005)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = Descent(0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 23197.097282215833\n",
      "1000 23118.569141432927\n",
      "1500 23063.923573414435\n",
      "2000 23075.693915605174\n",
      "2500 22841.44186243269\n",
      "3000 22774.829755003568\n",
      "3500 22588.70308742383\n",
      "4000 22415.333064564482\n",
      "4500 22305.83956907159\n",
      "5000 22175.20869814233\n",
      "5500 22178.79573760113\n",
      "6000 22001.094962003048\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] getindex",
      "    @ ./tuple.jl:29 [inlined]",
      "  [2] gradindex(x::Tuple{Nothing, NamedTuple{(:f, :x, :b), Tuple{Nothing, Vector{Float64}, Nothing}}, Nothing}, i::Int64)",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/reverse.jl:12",
      "  [3] Pullback",
      "    @ ./none:0 [inlined]",
      "  [4] (::typeof(∂(λ)))(Δ::Float64)",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface2.jl:0",
      "  [5] #551",
      "    @ ~/.julia/packages/Zygote/lwmfx/src/lib/array.jl:207 [inlined]",
      "  [6] (::Base.var\"#4#5\"{Zygote.var\"#551#555\"})(a::Tuple{typeof(∂(λ)), Float64})",
      "    @ Base ./generator.jl:36",
      "  [7] iterate",
      "    @ ./generator.jl:47 [inlined]",
      "  [8] collect_to!(dest::Vector{Tuple{NamedTuple{(:f, :x, :b), Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}}}, Nothing}}, itr::Base.Generator{Base.Iterators.Zip{Tuple{Vector{typeof(∂(λ))}, Vector{Float64}}}, Base.var\"#4#5\"{Zygote.var\"#551#555\"}}, offs::Int64, st::Tuple{Int64, Int64})",
      "    @ Base ./array.jl:724",
      "  [9] collect_to_with_first!(dest::Vector{Tuple{NamedTuple{(:f, :x, :b), Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}}}, Nothing}}, v1::Tuple{NamedTuple{(:f, :x, :b), Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}}}, Nothing}, itr::Base.Generator{Base.Iterators.Zip{Tuple{Vector{typeof(∂(λ))}, Vector{Float64}}}, Base.var\"#4#5\"{Zygote.var\"#551#555\"}}, st::Tuple{Int64, Int64})",
      "    @ Base ./array.jl:702",
      " [10] collect(itr::Base.Generator{Base.Iterators.Zip{Tuple{Vector{typeof(∂(λ))}, Vector{Float64}}}, Base.var\"#4#5\"{Zygote.var\"#551#555\"}})",
      "    @ Base ./array.jl:683",
      " [11] map",
      "    @ ./abstractarray.jl:2383 [inlined]",
      " [12] (::Zygote.var\"#550#554\"{Vector{typeof(∂(λ))}})(Δ::Vector{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/lib/array.jl:207",
      " [13] (::Zygote.var\"#581#582\"{Zygote.var\"#550#554\"{Vector{typeof(∂(λ))}}})(ȳ::Vector{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/lib/array.jl:239",
      " [14] Pullback",
      "    @ ~/.julia/packages/Binning/KnHZV/src/Binning.jl:68 [inlined]",
      " [15] (::typeof(∂(λ)))(Δ::Vector{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface2.jl:0",
      " [16] Pullback",
      "    @ ~/.julia/packages/Flux/qp1gc/src/layers/basic.jl:36 [inlined]",
      " [17] (::typeof(∂(applychain)))(Δ::Vector{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface2.jl:0",
      " [18] Pullback",
      "    @ ~/.julia/packages/Flux/qp1gc/src/layers/basic.jl:38 [inlined]",
      " [19] (::typeof(∂(λ)))(Δ::Vector{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface2.jl:0",
      " [20] Pullback",
      "    @ ./In[88]:2 [inlined]",
      " [21] (::typeof(∂(loss)))(Δ::Float64)",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface2.jl:0",
      " [22] Pullback",
      "    @ ./In[108]:6 [inlined]",
      " [23] (::typeof(∂(λ)))(Δ::Float64)",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface2.jl:0",
      " [24] (::Zygote.var\"#69#70\"{Zygote.Params, typeof(∂(λ)), Zygote.Context})(Δ::Float64)",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface.jl:252",
      " [25] gradient(f::Function, args::Zygote.Params)",
      "    @ Zygote ~/.julia/packages/Zygote/lwmfx/src/compiler/interface.jl:59",
      " [26] top-level scope",
      "    @ In[108]:5",
      " [27] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [28] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1094"
     ]
    }
   ],
   "source": [
    "ctr = 1\n",
    "for i ∈ 1:size(Xtrain)[2]\n",
    "    x = Xtrain[:, i]\n",
    "    y = Ytrain[:, i]\n",
    "    gs = Flux.gradient(ps_b) do \n",
    "        loss(x, y)    \n",
    "    end \n",
    "    Flux.Optimise.update!(opt, ps_b, gs)\n",
    "    if ctr%500 == 0\n",
    "        println(ctr, \" \", loss(x,y))\n",
    "    end\n",
    "    ctr += 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
